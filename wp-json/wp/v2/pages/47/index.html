{"id":47,"date":"2023-09-06T17:38:53","date_gmt":"2023-09-06T17:38:53","guid":{"rendered":"http:\/\/localhost:10023\/?page_id=47"},"modified":"2024-05-22T13:17:26","modified_gmt":"2024-05-22T13:17:26","slug":"research","status":"publish","type":"page","link":"http:\/\/localhost:10023\/research\/","title":{"rendered":"Research"},"content":{"rendered":"\n<p>[Contributors to this list of questions: Qian Cao, Susan Danziger, Jens Haas, Helen Han Wei Luo, Lucas Haugeberg, Anthony Dyson Hejduk, Wooseok Kim, Tahlia Pajaczkowska-Russell, Jonathan Tanaka, Katja Maria Vogt, Albert Wenger]<\/p>\n\n\n\n<p>v2024.05.12<\/p>\n\n\n\n<p><em>Agency<\/em><\/p>\n\n\n\n<p>\u2014 Are current models built as if the AI pursues<em> its<\/em> ends? If yes, does this speak for a radical shift, toward models oriented toward <em>human<\/em> final ends?<\/p>\n\n\n\n<p>\u2014 If AIs can have values or posited ends, should there be a built-in, strict dominance of human values to AI values?<\/p>\n\n\n\n<p>\u2014 Should AIs try to emulate that human decision-making is fundamentally concerned with sustaining human life and guided by what agents take to be well-lived human lives?<\/p>\n\n\n\n<p>\u2014 Who is responsible for decisions that the AI makes? Is the AI itself responsible, or are its creators responsible for the AI\u2019s decisions?<\/p>\n\n\n\n<p>\u2014 Is AI as a technology \u201cvalue neutral,\u201d simply reflective of the values of its creators and the data it is trained on? Alternatively, is it imbued with a bent toward, or away from, particular values?<\/p>\n\n\n\n<p><em>Ethical Values<\/em><\/p>\n\n\n\n<p>\u2014 Are there values, for example, related to the survival of humankind, that neither human beings nor intelligent machines should override?<\/p>\n\n\n\n<p>\u2014 What notion(s) of fairness do AI researchers employ?&nbsp;<\/p>\n\n\n\n<p>\u2014 When researchers describe AIs as fair or just, do they invoke agential or systemic notions? In other words, should we think of AIs as agents in the world, who can have virtues such as justice or fairness? Should we think of them as components of the social environments that constrain human action?<\/p>\n\n\n\n<p>\u2014 Can the special weight of moral considerations in human reasoning be simulated by AIs?<\/p>\n\n\n\n<p>\u2014 Ethics is concerned with how human beings should live. Does this mean that AIs should ask \u201cwhat should a human agent do?\u201d (as opposed to \u201cwhat should one do?\u201d).<\/p>\n\n\n\n<p>\u2014 Would it be appropriate for human beings to defer ethical decision-making to AIs? Does this manner of decision-making threaten human autonomy?<\/p>\n\n\n\n<p><em>Truth and Other Epistemic Values<\/em><\/p>\n\n\n\n<p>\u2014 What is the role of epistemic norms, for example, norms that request attention to evidence, careful thinking, etc., in AIs?<\/p>\n\n\n\n<p>\u2014 Is sensitivity to value an additional, separable dimension of AIs, to be added to existing systems? Alternatively, are \u201cethical abilities\u201d integrated dimensions of the \u201cthinking abilities\u201d of AIs, such that they improve along with them?<\/p>\n\n\n\n<p>\u2014 What is an LLM\u2019s relationship to the truth or falsity of its outputs? What does it mean for an LLM to \u201ctell the truth,\u201d \u201clie,\u201d \u201challucinate,\u201d etc.? Can AIs have virtues such as honesty?<\/p>\n\n\n\n<p><em>Credences and Risk<\/em><\/p>\n\n\n\n<p>\u2014 Suppose that AIs should be designed such that they assign probabilistically coherent credences to propositions or to surrogates for propositions. What ought to be the credence thresholds for action or belief reports?&nbsp;<\/p>\n\n\n\n<p>\u2014 What should guide AI designers with regard to credence thresholds? Risk aversion?<\/p>\n\n\n\n<p>\u2014 Should AIs be designed to primarily avoid assigning high credence to falsehoods, or should it primarily aim to assign high credences to truths?<\/p>\n\n\n\n<p>\u2014 Can we afford to design AIs to make mistakes from time to time, or should all high credence assignments result from extremely good epistemic positions?<\/p>\n\n\n\n<p><em>Mental States<\/em><\/p>\n\n\n\n<p>\u2014 We don\u2019t know whether AIs will ever have mental states such as intentions and beliefs. Lying arguably involves both: an intention to deceive and saying something one believes to be false. Is it a mere metaphor to describe AI-outputs in terms of truth-telling versus lying?&nbsp;<\/p>\n\n\n\n<p>\u2014 An AI-system may be said to have \u201cinformation.\u201d Does it \u201cbelieve\u201d the things it can provide as information? Does it \u201cknow\u201d them? What are the functional analogues to mental states such as belief and knowledge?<\/p>\n\n\n\n<p>\u2014 How do questions of interpretability bear on ethics? For example, are ethical questions ones where it is especially important to not only have the answer, but also to understand how the answer was generated?&nbsp;<\/p>\n\n\n\n<p>\u2014 Should AI models try to emulate the roles of emotion and desiderative\/aversive attitudes in human decision-making? If yes, how?<\/p>\n\n\n\n<p><em>Flawed Thinking<\/em><\/p>\n\n\n\n<p>\u2014 Should value integration start from the premise that intelligent machines ought to help us become better thinkers? How does this relate to the frequently-asked question of whether intelligent machines are, or will soon become, better thinkers than we are?<\/p>\n\n\n\n<p>\u2014 Via the corpora of text, images, etc., that LLMs ingest, they inherit flaws of human thinking, e.g., jumping to conclusions, fallacies. Which dimensions of human reasoning do we want to reproduce? Which dimensions of human reasoning can be improved with the help of machines?<\/p>\n\n\n\n<p>\u2014 In human beings, informal fallacies are often treated as \u201cshortcuts\u201d or \u201cfast track\u201d thinking, saving time and mental energy in resource limited environments. Are we aiming for AIs without any such \u201cshortcuts\u201d? If yes, this would constitute a major difference between human and machine reasoning.<\/p>\n\n\n\n<p><em>Pluralism and Disagreement<\/em><\/p>\n\n\n\n<p>\u2014 How can computational intelligence recognize value pluralism, disagreement, and historical changes in evaluative outlooks?<\/p>\n\n\n\n<p>\u2014 Is there a role for the use of different outputs from different LLMs? Do LLMs that are tailored to particular viewpoints generate \u201cecho chamber\u201d problems?<\/p>\n\n\n\n<p>\u2014 How can AIs recognize that human evaluative outlooks tend to be fragmented and inconsistent? Should they seek to correct these inconsistencies?<\/p>\n\n\n\n<p>\u2014 What is the effect of adding an extensive curriculum in ethics, including works that defend a range of approaches, to the training of an AI model?<\/p>\n","protected":false},"excerpt":{"rendered":"<p>[Contributors to this list of questions: Qian Cao, Susan Danziger, Jens Haas, Helen Han Wei Luo, Lucas Haugeberg, Anthony Dyson Hejduk, Wooseok Kim, Tahlia Pajaczkowska-Russell, Jonathan Tanaka, Katja Maria Vogt, Albert Wenger] v2024.05.12 Agency \u2014 Are current models built as if the AI pursues its ends? If yes, does this speak for a radical shift, [&hellip;]<\/p>\n","protected":false},"author":1,"featured_media":0,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"","meta":{"footnotes":""},"_links":{"self":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages\/47"}],"collection":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/comments?post=47"}],"version-history":[{"count":48,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages\/47\/revisions"}],"predecessor-version":[{"id":329,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages\/47\/revisions\/329"}],"wp:attachment":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/media?parent=47"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}