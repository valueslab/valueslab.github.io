{"id":47,"date":"2023-09-06T17:38:53","date_gmt":"2023-09-06T17:38:53","guid":{"rendered":"http:\/\/localhost:10023\/?page_id=47"},"modified":"2023-11-12T13:10:49","modified_gmt":"2023-11-12T13:10:49","slug":"research","status":"publish","type":"page","link":"http:\/\/localhost:10023\/research\/","title":{"rendered":"Research"},"content":{"rendered":"\n<p>Research questions include:<\/p>\n\n\n\n<p>\u2014 Are there sets of values, for example, related to the survival of humankind, that can be considered overriding, such that intelligent machines should not override them?<\/p>\n\n\n\n<p>\u2014 How can computational intelligence recognize value pluralism, disagreement, and historical changes in evaluative outlooks?<\/p>\n\n\n\n<p>\u2014 How can computational intelligence recognize that human evaluative outlooks tend to be fragmented and inconsistent, a mix of values, principles, and deep-seated affective attitudes?<\/p>\n\n\n\n<p>\u2014 In response to the question \u201cwhich values?\u201d, the ValuesLab starts from the notion of human values. This includes ethical and moral values such as justice, respect for persons, etc., but also epistemic values related to better thinking.<\/p>\n\n\n\n<p>\u2014 The ValuesLab explores options for deep integration of human values and AI that starts from epistemic norms, norms such as \u201clet\u2019s think about this carefully\u201d; studies suggest that the integration of such norms into algorithms improves responses (Wei et al. 2022).<\/p>\n\n\n\n<p>\u2014 The ValuesLab explores options for value integration that start from a Socratic, question-based model, such that AI pushes human users toward better prompts (and, derivatively, better responses).<\/p>\n\n\n\n<p>\u2014 The ValuesLab asks how questions of interpretability of AI systems bear on ethics; for example, are ethical\/moral questions ones where it is especially important to not only have the answer, but also understand how to arrive at the answer?<\/p>\n\n\n\n<p>\u2014 What is the effect of adding an extensive curriculum in ethics to the training of an AI model?<\/p>\n\n\n\n<p>\u2014 Can existing training content be enhanced by adding material that focuses on moral\/ethical values?<\/p>\n\n\n\n<p>\u2014 Is there an analogue to responsibility\/accountability for decisions by AI models?<\/p>\n\n\n\n<p>\u2014 Can the special salience of moral considerations in human reasoning be simulated by attention mechanisms?<\/p>\n\n\n\n<p>\u2014 What is an LLM\u2019s relationship to the truth or falsity of its outputs? What does it mean for an LLM to \u201ctell the truth,\u201d \u201clie,\u201d \u201challucinate,\u201d etc.?<\/p>\n\n\n\n<p>The ValuesLab starts from the assumption that integration of human values and computational intelligence requires reflection on the human-machine relationship. Some questions:<\/p>\n\n\n\n<p>\u2014 Ethics, as a field, asks how human beings should live and act. Does this mean that AI models should ask \u201cwhat should a human agent do?\u201d (as opposed to, more neutrally, \u201cwhat should one do?\u201d)<\/p>\n\n\n\n<p>\u2014 Should AI models try to emulate the roles of emotion, desiderative\/aversive attitudes in human decision-making? If yes, how?<\/p>\n\n\n\n<p>\u2014 Should AI models try to emulate that human decision-making is fundamentally concerned with sustaining human life and acting \u201cunder the guise of the good,\u201d in particular, under the guise of what agents take to be well-lived human lives?<\/p>\n\n\n\n<p>\u2014 Should value integration start from the premise that intelligent machines help us to be better thinkers? How does this premise relate to the much-asked question of whether intelligent machines are, or soon are, better thinkers than we are?<\/p>\n\n\n\n<p>\u2014 So far, human-machine relationships are typically thought to be asymmetrical and hierarchical, with humans as users, builders, and intended beneficiaries. Can there be symmetrical and non-hierarchical human-machine relationships? What about scenarios that envisage humans as subservient to machines?<\/p>\n\n\n\n<p>\u2014 Via the corpora of text, images, etc., that LLMs \u201cingest,\u201d they inherit flaws of human thinking, e.g., jumping to conclusions, fallacies. Which dimensions of human reasoning do we want to reproduce? Which dimensions of human reasoning can be improved with the help of machines?<\/p>\n","protected":false},"excerpt":{"rendered":"<p>Research questions include: \u2014 Are there sets of values, for example, related to the survival of humankind, that can be considered overriding, such that intelligent machines should not override them? \u2014 How can computational intelligence recognize value pluralism, disagreement, and historical changes in evaluative outlooks? \u2014 How can computational intelligence recognize that human evaluative outlooks [&hellip;]<\/p>\n","protected":false},"author":1,"featured_media":0,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"","meta":{"footnotes":""},"_links":{"self":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages\/47"}],"collection":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/comments?post=47"}],"version-history":[{"count":9,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages\/47\/revisions"}],"predecessor-version":[{"id":154,"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/pages\/47\/revisions\/154"}],"wp:attachment":[{"href":"http:\/\/localhost:10023\/wp-json\/wp\/v2\/media?parent=47"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}